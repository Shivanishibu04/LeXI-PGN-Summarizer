# ‚è±Ô∏è Training Time Guide & Speed Optimization

## üìä Your Dataset

- **Training examples**: 7,030 legal documents
- **File size**: 210 MB
- **Average document length**: ~30KB per document

---

## ‚è∞ Training Time Estimates (CPU)

### Option 1: **FULL Training** (`train.py`)

```
Configuration:
  ‚Ä¢ Full dataset: 7,030 examples
  ‚Ä¢ Model size: ~117M parameters
  ‚Ä¢ Vocabulary: 50,000 tokens
  ‚Ä¢ Epochs: 20
  ‚Ä¢ Extractive top-k: 10 sentences
  
‚è±Ô∏è Time per epoch: 3-5 hours
üìÖ Total time: 60-100 hours (2.5-4 days)
```

**Pros**: Best quality results  
**Cons**: Very slow on CPU ‚ö†Ô∏è

---

### Option 2: **FAST Training** (`train_fast.py`) ‚úÖ RECOMMENDED

```
Configuration:
  ‚Ä¢ Sampled dataset: 1,054 examples (15%)
  ‚Ä¢ Model size: ~25M parameters (smaller)
  ‚Ä¢ Vocabulary: 20,000 tokens
  ‚Ä¢ Epochs: 10 (reduced)
  ‚Ä¢ Extractive top-k: 5 sentences (shorter)
  
‚è±Ô∏è Time per epoch: 6-12 minutes
üìÖ Total time: 1-2 hours
```

**Pros**: 30-50x faster! ‚úÖ  
**Cons**: Slightly lower quality (still good)  

---

### Option 3: **QUICK TEST** (Manual config)

```
Configuration:
  ‚Ä¢ Tiny dataset: 200 examples (3%)
  ‚Ä¢ Model size: ~10M parameters (very small)
  ‚Ä¢ Vocabulary: 10,000 tokens
  ‚Ä¢ Epochs: 5
  ‚Ä¢ Extractive top-k: 3 sentences
  
‚è±Ô∏è Time per epoch: 2-3 minutes
üìÖ Total time: 10-15 minutes
```

**Pros**: Ultra fast for testing  
**Cons**: Demo quality only  

---

## üöÄ Speed Optimization Strategies

### What We Changed in `config_fast.py`:

| Setting | Original | Fast | Impact | 
|---------|----------|------|---------|
| **Dataset** | 7,030 (100%) | 1,054 (15%) | 7x faster ‚ö° |
| **Model size** | 117M params | 25M params | 4x faster ‚ö° |
| **Vocab size** | 50,000 | 20,000 | 2x faster ‚ö° |
| **Encoder layers** | 2 | 1 | 1.5x faster ‚ö° |
| **Hidden dim** | 512 | 256 | 2x faster ‚ö° |
| **Embedding** | 256 | 128 | 1.3x faster ‚ö° |
| **Max encoder len** | 512 | 256 | 2x faster ‚ö° |
| **Extractive top-k** | 10 sent. | 5 sent. | 1.5x faster ‚ö° |
| **Batch size** | 8 | 16 | 2x faster ‚ö° |
| **Epochs** | 20 | 10 | 2x faster ‚ö° |

**Combined speedup: ~40-50x faster** üöÄ

---

## üìã How to Use

### **For Quick Results (RECOMMENDED):**

```powershell
# Use the FAST training script
python train_fast.py
```

**Expected output:**
```
FAST Configuration loaded. Device: cpu
Model size: ~25M parameters (vs 117M in full config)
Dataset: 15% sample (1054 examples)
Estimated training time: 1-2 hours on CPU

Epoch 1/10
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [08:23<00:00]
  Total loss: 5.234
  Time: 8m 23s

Validating...
  Total loss: 4.982

‚úì New best validation loss
```

---

### **For Best Quality (if you have time):**

```powershell
# Use the full training script
python train.py
```

**Note**: This will take 2-4 DAYS on CPU. Consider:
- Running overnight
- Using GPU if available
- Or stick with fast mode ‚úÖ

---

## üí° Additional Speed Tips

### 1. **Reduce Dataset Further** (if still too slow)

Edit `config_fast.py`:
```python
SAMPLE_FRACTION = 0.05  # Use only 5% (~350 examples)
```

### 2. **Make Model Even Smaller**

Edit `config_fast.py`:
```python
EMBEDDING_DIM = 64          # Was 128
ENCODER_HIDDEN_DIM = 128    # Was 256
DECODER_HIDDEN_DIM = 128    # Was 256
VOCAB_SIZE = 10000          # Was 20000
```

### 3. **Shorten Sequences More**

Edit `config_fast.py`:
```python
MAX_ENCODER_LEN = 128       # Was 256
MAX_DECODER_LEN = 50        # Was 100
EXTRACTIVE_TOP_K = 3        # Was 5
```

### 4. **Use Fewer Epochs**

Edit `config_fast.py`:
```python
NUM_EPOCHS = 5              # Was 10
```

---

## ‚öñÔ∏è Quality vs Speed Trade-off

| Mode | Time | Quality | Use Case |
|------|------|---------|----------|
| **Full** | 60-100h | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Production/Research |
| **Fast** | 1-2h | ‚≠ê‚≠ê‚≠ê‚≠ê | Quick experiments ‚úÖ |
| **Quick** | 10-15m | ‚≠ê‚≠ê‚≠ê | Testing pipeline |

---

## üìà Performance Expectations

### Fast Mode Results:
- **Extractive**: Excellent (using your CNN-CRF + SentenceSummarizer)
- **Abstractive**: Good quality summaries
- **Coherence**: High
- **Factuality**: Good (copy mechanism helps)
- **Repetition**: Minimal (coverage loss)

### What you'll get:
```
Input (after extractive):
"The charge created in respect of municipal property... 
The Revenue Recovery Act must be construed..."

Generated Summary:
"Municipal tax charges are not considered government 
revenue under the Revenue Recovery Act."
```

---

## üéØ Recommendation

**For your use case (CPU training):**

1. **START HERE**: `python train_fast.py` ‚úÖ
   - 1-2 hours total
   - Good quality results
   - Tests the full pipeline

2. **If satisfied**: Keep using fast mode or scale up later

3. **If you need production quality**: 
   - Consider using GPU (cloud/colab)
   - Or run full training overnight for several days

---

## üñ•Ô∏è GPU vs CPU Comparison

If you had a GPU available:

| Configuration | CPU | GPU (RTX 3060) |
|---------------|-----|----------------|
| **Full training** | 60-100h | 4-6h |
| **Fast training** | 1-2h | 8-12 min |

**GPU is 10-15x faster than CPU**

Free GPU options:
- Google Colab (free tier: 12h sessions)
- Kaggle (30h/week free GPU)

---

## ‚úÖ Ready to Start?

**Recommended command:**
```powershell
python train_fast.py
```

**Monitor progress:**
- Watch the progress bars
- Check validation loss decreasing
- First epoch shows time-per-epoch estimate

**Stop early if needed:**
- Press `Ctrl+C` to stop
- Model checkpoints are saved every 2 epochs
- You can resume later (manually load checkpoint)

---

**Want to proceed with fast training?** üöÄ
